{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd71d8ce-b99d-4cb7-a29e-276e55c9d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#纯使用BERT模型版本\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def extract_pooled_keywords():\n",
    "    extract_keywords(method=\"pooled_output\")\n",
    "\n",
    "def extract_sequence_keywords():\n",
    "    extract_keywords(method=\"sequence_output\")\n",
    "\n",
    "def extract_keywords(method):\n",
    "    # 打开文件对话框，让用户选择要上传的文件\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "    \n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 读取用户选择的文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # 将文本数据转换为适合 BERT 模型输入的格式\n",
    "        inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        # 输入文本数据到 BERT 模型中\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        keywords = []\n",
    "\n",
    "        if method == \"pooled_output\":\n",
    "            pooled_output = outputs.pooler_output.numpy()[0]\n",
    "            word_ids = inputs['input_ids'][0].numpy()\n",
    "            special_tokens = {tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id}\n",
    "            id2word = {idx: tokenizer.convert_ids_to_tokens([word_id])[0] for idx, word_id in enumerate(word_ids) if word_id not in special_tokens}\n",
    "            keywords = [(id2word[idx], pooled_output[idx % len(pooled_output)]) for idx in id2word]\n",
    "\n",
    "        elif method == \"sequence_output\":\n",
    "            sequence_output = outputs.last_hidden_state.numpy()[0]\n",
    "            word_ids = inputs['input_ids'][0].numpy()\n",
    "            special_tokens = {tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id}\n",
    "            id2word = {idx: tokenizer.convert_ids_to_tokens([word_id])[0] for idx, word_id in enumerate(word_ids) if word_id not in special_tokens}\n",
    "            word_scores = np.mean(sequence_output, axis=1)\n",
    "            keywords = [(id2word[idx], word_scores[idx]) for idx in id2word]\n",
    "\n",
    "        # 根据重要性分数降序排列关键词\n",
    "        keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 将提取的关键词结果显示在文本框中\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for keyword, score in keywords:\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "# 创建主窗口\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"500x400\")  # 设置窗口尺寸\n",
    "\n",
    "# 创建一个框架，用于存放按钮\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "# 创建按钮选择不同的提取方式\n",
    "pooled_button = tk.Button(button_frame, text=\"使用池化输出提取\", command=extract_pooled_keywords, width=20)\n",
    "pooled_button.grid(row=0, column=0, padx=5)\n",
    "\n",
    "sequence_button = tk.Button(button_frame, text=\"使用序列输出提取\", command=extract_sequence_keywords, width=20)\n",
    "sequence_button.grid(row=0, column=1, padx=5)\n",
    "\n",
    "# 创建显示结果的文本框并添加滚动条\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# 运行主循环\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607021a6-502e-4bc0-b43c-05b8e4bccc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.8.4.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentence-transformers>=0.3.8 (from keybert)\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keybert) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keybert) (1.26.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.40.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.66.2)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading torch-2.3.0-cp39-cp39-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (4.9.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.3)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.4.3)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.2.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/171.5 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 61.4/171.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 171.5/171.5 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.3.0-cp39-cp39-win_amd64.whl (159.7 MB)\n",
      "   ---------------------------------------- 0.0/159.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/159.7 MB 4.8 MB/s eta 0:00:34\n",
      "   ---------------------------------------- 0.5/159.7 MB 5.4 MB/s eta 0:00:30\n",
      "   ---------------------------------------- 1.1/159.7 MB 8.1 MB/s eta 0:00:20\n",
      "    --------------------------------------- 2.1/159.7 MB 11.0 MB/s eta 0:00:15\n",
      "    --------------------------------------- 3.0/159.7 MB 12.9 MB/s eta 0:00:13\n",
      "    --------------------------------------- 4.0/159.7 MB 14.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.0/159.7 MB 15.2 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 6.0/159.7 MB 16.1 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 7.1/159.7 MB 16.9 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 8.1/159.7 MB 17.3 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 9.2/159.7 MB 17.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 10.1/159.7 MB 18.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.1/159.7 MB 21.1 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 12.1/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 13.1/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 14.2/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 15.2/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 16.2/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 17.3/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 18.4/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 19.4/159.7 MB 21.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 20.4/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 21.5/159.7 MB 21.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 22.5/159.7 MB 21.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 23.5/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 24.6/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 25.6/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 26.7/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 27.7/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 28.8/159.7 MB 21.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 29.8/159.7 MB 21.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 30.8/159.7 MB 21.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 31.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 32.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 33.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 34.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 35.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 36.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 37.8/159.7 MB 21.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 38.8/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 40.0/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 41.0/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 41.9/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 42.8/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 43.9/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 44.8/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 45.8/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 46.9/159.7 MB 21.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 47.9/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 49.0/159.7 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 49.9/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 50.9/159.7 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 51.9/159.7 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 52.9/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 53.9/159.7 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 54.9/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 55.9/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 56.9/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 58.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 59.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 60.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 61.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 62.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 63.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 64.1/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 65.0/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 66.1/159.7 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 67.3/159.7 MB 21.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 68.2/159.7 MB 21.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 69.3/159.7 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 70.3/159.7 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 71.3/159.7 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 72.3/159.7 MB 21.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 73.4/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 74.4/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 75.4/159.7 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 76.5/159.7 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 77.5/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 78.4/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 79.4/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 80.5/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 81.6/159.7 MB 22.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 82.7/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 83.7/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 84.8/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 85.8/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 86.8/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 87.8/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 88.9/159.7 MB 21.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 89.9/159.7 MB 21.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 90.9/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 91.8/159.7 MB 21.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 92.9/159.7 MB 21.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 93.9/159.7 MB 21.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 95.0/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 96.1/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 97.1/159.7 MB 21.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 98.1/159.7 MB 21.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 99.2/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 100.1/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 101.2/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 102.2/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 103.2/159.7 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 104.3/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 105.3/159.7 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 106.3/159.7 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 107.3/159.7 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 108.3/159.7 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 109.4/159.7 MB 21.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 110.4/159.7 MB 21.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 111.4/159.7 MB 21.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 112.4/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 113.5/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 114.6/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 115.6/159.7 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 116.6/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 117.6/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 118.7/159.7 MB 21.9 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 119.6/159.7 MB 21.9 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 120.6/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 121.8/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 122.8/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 123.9/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 124.9/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 125.8/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 126.9/159.7 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 127.9/159.7 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 129.0/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 129.9/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 130.9/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 131.9/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 132.9/159.7 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 133.8/159.7 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 134.9/159.7 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 135.9/159.7 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 137.0/159.7 MB 21.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 138.1/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 139.1/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 140.0/159.7 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 141.1/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 142.1/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 143.1/159.7 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 144.1/159.7 MB 21.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.1/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 146.2/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 147.3/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 148.5/159.7 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.5/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 150.7/159.7 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 151.9/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 153.0/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 154.1/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 155.2/159.7 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.3/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  157.4/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  158.5/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.6/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.7/159.7 MB 15.6 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/228.5 MB 17.0 MB/s eta 0:00:14\n",
      "   ---------------------------------------- 1.6/228.5 MB 17.2 MB/s eta 0:00:14\n",
      "   ---------------------------------------- 2.4/228.5 MB 17.3 MB/s eta 0:00:14\n",
      "    --------------------------------------- 3.3/228.5 MB 17.4 MB/s eta 0:00:13\n",
      "    --------------------------------------- 4.2/228.5 MB 17.8 MB/s eta 0:00:13\n",
      "    --------------------------------------- 5.2/228.5 MB 18.6 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 6.3/228.5 MB 19.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 7.3/228.5 MB 19.4 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 8.3/228.5 MB 19.7 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 9.3/228.5 MB 19.8 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 10.4/228.5 MB 19.8 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 11.4/228.5 MB 20.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 12.3/228.5 MB 20.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 13.2/228.5 MB 21.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 14.1/228.5 MB 21.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 15.1/228.5 MB 21.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 16.1/228.5 MB 21.1 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 17.2/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 18.2/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 19.2/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 20.0/228.5 MB 20.5 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 21.0/228.5 MB 20.5 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 21.9/228.5 MB 20.5 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 22.9/228.5 MB 20.5 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 24.0/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 25.0/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 26.0/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 27.0/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 27.9/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 28.9/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 29.8/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 30.7/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 31.5/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 32.5/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 33.5/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 34.5/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 35.5/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 36.6/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 37.6/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 38.7/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 39.7/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 40.8/228.5 MB 21.8 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 41.9/228.5 MB 21.8 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 42.8/228.5 MB 21.8 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 43.8/228.5 MB 21.8 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 44.7/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 45.6/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 46.5/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 47.6/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 48.7/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 49.8/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 50.9/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 51.9/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 53.0/228.5 MB 21.8 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 54.1/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 55.2/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 56.2/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 57.4/228.5 MB 23.4 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 58.3/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 59.3/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 60.3/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 61.2/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 62.0/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 63.0/228.5 MB 21.9 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 63.9/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 64.9/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 66.0/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 67.0/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 68.1/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 69.0/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 70.1/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 71.1/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 72.2/228.5 MB 21.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 73.2/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 74.1/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 75.2/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 76.2/228.5 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 77.2/228.5 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 78.3/228.5 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 79.3/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 80.2/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 81.1/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 82.0/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 83.0/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 84.0/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 85.0/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 85.9/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 86.9/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 88.0/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 89.1/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 90.1/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 91.2/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 92.1/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 93.0/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 94.1/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 95.1/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 96.2/228.5 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 97.2/228.5 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 98.3/228.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 99.2/228.5 MB 21.1 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 100.3/228.5 MB 21.9 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 101.3/228.5 MB 21.9 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 102.3/228.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 103.2/228.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 104.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 105.0/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 105.9/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 106.9/228.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 107.9/228.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 109.0/228.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 110.0/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 111.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 112.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 113.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 114.2/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 115.1/228.5 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 116.2/228.5 MB 21.9 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 117.2/228.5 MB 21.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 118.2/228.5 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 119.2/228.5 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 120.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 121.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 122.2/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 123.2/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 124.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 125.4/228.5 MB 21.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 126.4/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 127.4/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 128.3/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 129.4/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 130.3/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 131.3/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 132.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 133.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 134.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 135.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 136.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 137.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 138.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 139.5/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 140.4/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 141.4/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 142.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 143.3/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 144.4/228.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 145.4/228.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 146.4/228.5 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 147.3/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 148.4/228.5 MB 21.9 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 149.3/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 150.3/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 151.3/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 152.4/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 153.4/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 154.5/228.5 MB 21.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 155.6/228.5 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 156.6/228.5 MB 21.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 157.6/228.5 MB 21.9 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 158.8/228.5 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 159.7/228.5 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 160.7/228.5 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 161.8/228.5 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 162.8/228.5 MB 21.8 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 163.7/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 164.8/228.5 MB 21.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 165.9/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 167.0/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 168.1/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 169.2/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 170.1/228.5 MB 21.9 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 171.2/228.5 MB 21.9 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 172.2/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 173.1/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 174.3/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 175.2/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 176.3/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 177.2/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 178.3/228.5 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 179.2/228.5 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 180.3/228.5 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 181.4/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 182.3/228.5 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 183.4/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 184.5/228.5 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 185.5/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 186.5/228.5 MB 21.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 187.5/228.5 MB 21.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 188.6/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 189.5/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 190.5/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 191.6/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 192.5/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 193.5/228.5 MB 21.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 194.5/228.5 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 195.6/228.5 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 196.6/228.5 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 197.5/228.5 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 198.5/228.5 MB 21.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 199.4/228.5 MB 21.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 200.3/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 201.3/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 202.2/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 203.2/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 204.1/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 205.1/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 206.1/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 207.0/228.5 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 207.7/228.5 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 208.7/228.5 MB 19.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 209.7/228.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 210.6/228.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 211.6/228.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 212.6/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 213.5/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 214.4/228.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 215.4/228.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 216.5/228.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 217.6/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 218.6/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 219.7/228.5 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 220.8/228.5 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 221.8/228.5 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  222.9/228.5 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  224.1/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  225.1/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  226.1/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  227.2/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.7/3.5 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.4/3.5 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.3/3.5 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.2/3.5 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.4/286.4 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.7/1.6 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 14.9 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.6/5.7 MB 20.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.4/5.7 MB 18.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.2/5.7 MB 17.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.1/5.7 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.1/5.7 MB 18.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.1/5.7 MB 19.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.7/5.7 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 17.5 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: keybert\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.8.4-py3-none-any.whl size=39256 sha256=bb7a290e332bb50c4761cdfb96f94bbc9fa5b0dd5d0ae6dfb38a920e9af50afa\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\92\\ba\\d5\\3ee7ff1730013a35be73dc895542a516c3a196cbf96ecbea32\n",
      "Successfully built keybert\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, torch, sentence-transformers, keybert\n",
      "Successfully installed intel-openmp-2021.4.0 keybert-0.8.4 mkl-2021.4.0 mpmath-1.3.0 networkx-3.2.1 sentence-transformers-2.7.0 sympy-1.12 tbb-2021.12.0 torch-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f2271f-3018-432a-bd9d-de6b8d289e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#BERT模型仿照KeyBERT提取的改进版\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def extract_keywords():\n",
    "    # 获取用户输入的关键词提取长度和数量\n",
    "    try:\n",
    "        ngram_range = int(ngram_entry.get())\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    # 打开文件对话框，让用户选择要上传的文件\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "    \n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 读取用户选择的文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # 将文本数据转换为适合 BERT 模型输入的格式\n",
    "        inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        # 输入文本数据到 BERT 模型中\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 获取最后一层的序列输出，并限制大小\n",
    "        sequence_output = outputs.last_hidden_state.numpy()[0]\n",
    "\n",
    "        # 获取输入的 token id 和映射的词汇\n",
    "        word_ids = inputs['input_ids'][0].numpy()\n",
    "        special_tokens = {tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id}\n",
    "        id2word = {idx: tokenizer.convert_ids_to_tokens([word_id])[0] for idx, word_id in enumerate(word_ids) if word_id not in special_tokens}\n",
    "\n",
    "        # 修复可能的索引超出错误：确保 `sequence_output` 和 `id2word` 索引一致\n",
    "        valid_indices = [idx for idx in id2word if idx < len(sequence_output)]\n",
    "        word_vectors = np.array([sequence_output[idx] for idx in valid_indices])\n",
    "        id2word = {idx: id2word[idx] for idx in valid_indices}\n",
    "\n",
    "        # 构建词向量与文本嵌入的余弦相似度\n",
    "        doc_vector = np.mean(sequence_output, axis=0).reshape(1, -1)\n",
    "        similarities = cosine_similarity(word_vectors, doc_vector).flatten()\n",
    "\n",
    "        # 将关键词根据相似度排序\n",
    "        keywords = sorted([(id2word[idx], similarities[i]) for i, idx in enumerate(valid_indices)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 显示提取的关键词（限制数量）\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            if i >= num_keywords:\n",
    "                break\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "# 创建主窗口\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"500x500\")  # 设置窗口尺寸\n",
    "\n",
    "# 创建一个框架，用于存放设置项\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "\n",
    "# 创建用户输入 ngram 范围和关键词数量的输入框\n",
    "ngram_label = tk.Label(settings_frame, text=\"关键词长度 (n-gram):\")\n",
    "ngram_label.grid(row=0, column=0, padx=5)\n",
    "ngram_entry = tk.Entry(settings_frame, width=5)\n",
    "ngram_entry.insert(0, \"1\")\n",
    "ngram_entry.grid(row=0, column=1, padx=5)\n",
    "\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "# 创建按钮进行关键词提取\n",
    "extract_button = tk.Button(root, text=\"提取关键词\", command=extract_keywords, width=20)\n",
    "extract_button.pack(pady=10)\n",
    "\n",
    "# 创建显示结果的文本框并添加滚动条\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# 运行主循环\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1a23b7-e5e8-4ae7-8eb2-68810a6315ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Sentence-transformer库版本\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 加载预训练的 Sentence Transformer 模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_keywords():\n",
    "    # 获取用户输入的关键词长度范围和数量\n",
    "    try:\n",
    "        ngram_range = int(ngram_entry.get())\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    # 打开文件对话框，让用户选择要上传的文件\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "    \n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 读取用户选择的文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # 使用 CountVectorizer 提取词组\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, ngram_range), stop_words='english').fit([text])\n",
    "        candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # 对每个候选词组进行嵌入计算\n",
    "        candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "        # 计算文档整体嵌入作为查询向量\n",
    "        doc_embedding = model.encode([text]).reshape(1, -1)\n",
    "\n",
    "        # 计算每个候选词组与文档的余弦相似度\n",
    "        similarities = cosine_similarity(candidate_embeddings, doc_embedding).flatten()\n",
    "\n",
    "        # 根据相似度对候选词组排序\n",
    "        keywords = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 显示提取的关键词（限制数量）\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            if i >= num_keywords:\n",
    "                break\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "# 创建主窗口\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"500x500\")  # 设置窗口尺寸\n",
    "\n",
    "# 创建一个框架，用于存放设置项\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "\n",
    "# 创建用户输入 ngram 范围和关键词数量的输入框\n",
    "ngram_label = tk.Label(settings_frame, text=\"关键词长度 (n-gram):\")\n",
    "ngram_label.grid(row=0, column=0, padx=5)\n",
    "ngram_entry = tk.Entry(settings_frame, width=5)\n",
    "ngram_entry.insert(0, \"2\")  # 默认2词\n",
    "ngram_entry.grid(row=0, column=1, padx=5)\n",
    "\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "# 创建按钮进行关键词提取\n",
    "extract_button = tk.Button(root, text=\"提取关键词\", command=extract_keywords, width=20)\n",
    "extract_button.pack(pady=10)\n",
    "\n",
    "# 创建显示结果的文本框并添加滚动条\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# 运行主循环\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ba8f83-f59d-427b-92fd-fdb54d0f5ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#同时使用两种模型的版本\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 加载Sentence Transformer模型\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_keywords_with_bert(text, num_keywords):\n",
    "    \"\"\"使用BERT模型提取关键词\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=512)\n",
    "    outputs = bert_model(inputs)\n",
    "    sequence_output = outputs.last_hidden_state.numpy()[0]\n",
    "\n",
    "    word_ids = inputs['input_ids'][0].numpy()\n",
    "    special_tokens = {tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id}\n",
    "    id2word = {idx: tokenizer.convert_ids_to_tokens([word_id])[0] for idx, word_id in enumerate(word_ids) if word_id not in special_tokens}\n",
    "\n",
    "    valid_indices = [idx for idx in id2word if idx < len(sequence_output)]\n",
    "    word_vectors = np.array([sequence_output[idx] for idx in valid_indices])\n",
    "    id2word = {idx: id2word[idx] for idx in valid_indices}\n",
    "\n",
    "    doc_vector = np.mean(sequence_output, axis=0).reshape(1, -1)\n",
    "    similarities = cosine_similarity(word_vectors, doc_vector).flatten()\n",
    "\n",
    "    keywords = sorted([(id2word[idx], similarities[i]) for i, idx in enumerate(valid_indices)], key=lambda x: x[1], reverse=True)\n",
    "    return keywords[:num_keywords]\n",
    "\n",
    "def extract_keywords_with_sentence_transformer(text, num_keywords):\n",
    "    \"\"\"使用Sentence Transformer模型提取关键词\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "    doc_vector = np.mean(embeddings, axis=0).reshape(1, -1)\n",
    "    similarities = cosine_similarity(embeddings, doc_vector).flatten()\n",
    "\n",
    "    keywords = sorted(zip(sentences, similarities), key=lambda x: x[1], reverse=True)\n",
    "    return keywords[:num_keywords]\n",
    "\n",
    "def extract_keywords():\n",
    "    try:\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        method = extraction_method.get()\n",
    "        if method == \"BERT\":\n",
    "            keywords = extract_keywords_with_bert(text, num_keywords)\n",
    "        else:\n",
    "            keywords = extract_keywords_with_sentence_transformer(text, num_keywords)\n",
    "\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"600x600\")\n",
    "\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "\n",
    "extraction_method = tk.StringVar(value=\"BERT\")\n",
    "\n",
    "bert_radio = tk.Radiobutton(settings_frame, text=\"BERT\", variable=extraction_method, value=\"BERT\")\n",
    "bert_radio.grid(row=0, column=0, padx=5)\n",
    "\n",
    "sentence_transformer_radio = tk.Radiobutton(settings_frame, text=\"Sentence Transformer\", variable=extraction_method, value=\"Sentence Transformer\")\n",
    "sentence_transformer_radio.grid(row=0, column=1, padx=5)\n",
    "\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "extract_button = tk.Button(root, text=\"提取关键词\", command=extract_keywords, width=20)\n",
    "extract_button.pack(pady=10)\n",
    "\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=20, width=70, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440eee52-9518-43d6-9468-14f236ce2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将Sentence-Transformer库包装后使用\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 加载预训练的 Sentence Transformer 模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_keywords():\n",
    "    # 获取用户输入的关键词长度范围和数量\n",
    "    try:\n",
    "        ngram_range = int(ngram_entry.get())\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    # 打开文件对话框，让用户选择要上传的文件\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 读取用户选择的文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # 使用 CountVectorizer 提取词组\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, ngram_range), stop_words='english').fit([text])\n",
    "        candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # 对每个候选词组进行嵌入计算\n",
    "        candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "        # 计算文档整体嵌入作为查询向量\n",
    "        doc_embedding = model.encode([text]).reshape(1, -1)\n",
    "\n",
    "        # 计算每个候选词组与文档的余弦相似度\n",
    "        similarities = cosine_similarity(candidate_embeddings, doc_embedding).flatten()\n",
    "\n",
    "        # 根据相似度对候选词组排序\n",
    "        keywords = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 显示提取的关键词（限制数量）\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            if i >= num_keywords:\n",
    "                break\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "# 创建主窗口\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"500x500\")  # 设置窗口尺寸\n",
    "\n",
    "# 创建一个框架，用于存放设置项\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "\n",
    "# 创建用户输入 ngram 范围和关键词数量的输入框\n",
    "ngram_label = tk.Label(settings_frame, text=\"关键词长度 (n-gram):\")\n",
    "ngram_label.grid(row=0, column=0, padx=5)\n",
    "ngram_entry = tk.Entry(settings_frame, width=5)\n",
    "ngram_entry.insert(0, \"2\")  # 默认2词\n",
    "ngram_entry.grid(row=0, column=1, padx=5)\n",
    "\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "# 创建按钮进行关键词提取\n",
    "extract_button = tk.Button(root, text=\"提取关键词\", command=extract_keywords, width=20)\n",
    "extract_button.pack(pady=10)\n",
    "\n",
    "# 创建显示结果的文本框并添加滚动条\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# 运行主循环\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfb6c786-d1bc-4160-a236-92016df2c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-hub in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-hub) (2.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow tensorflow-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "930efe04-6d3a-423c-a203-d552857caf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 加载预训练的 Universal Sentence Encoder 模型\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def extract_keywords():\n",
    "    try:\n",
    "        ngram_range = int(ngram_entry.get())\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, ngram_range), stop_words='english').fit([text])\n",
    "        candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "        candidate_embeddings = embed(candidates).numpy()\n",
    "        doc_embedding = embed([text]).numpy().reshape(1, -1)\n",
    "\n",
    "        similarities = cosine_similarity(candidate_embeddings, doc_embedding).flatten()\n",
    "        keywords = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            if i >= num_keywords:\n",
    "                break\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"500x500\")\n",
    "\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "ngram_label = tk.Label(settings_frame, text=\"关键词长度 (n-gram):\")\n",
    "ngram_label.grid(row=0, column=0, padx=5)\n",
    "ngram_entry = tk.Entry(settings_frame, width=5)\n",
    "ngram_entry.insert(0, \"2\")\n",
    "ngram_entry.grid(row=0, column=1, padx=5)\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "extract_button = tk.Button(root, text=\"提取关键词\", command=extract_keywords, width=20)\n",
    "extract_button.pack(pady=10)\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2de5d2-9214-4c84-a6aa-32df0715f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Universal Sentence Encoder的模型\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 加载预训练的 Universal Sentence Encoder 模型\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def extract_keywords():\n",
    "    # 获取用户输入的关键词长度范围和数量\n",
    "    try:\n",
    "        ngram_range = int(ngram_entry.get())\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    # 打开文件对话框，让用户选择要上传的文件\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 读取用户选择的文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # 使用 CountVectorizer 提取词组\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, ngram_range), stop_words='english').fit([text])\n",
    "        candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # 对每个候选词组进行嵌入计算\n",
    "        candidate_embeddings = embed(candidates)\n",
    "\n",
    "        # 计算文档整体嵌入作为查询向量\n",
    "        doc_embedding = embed([text])\n",
    "\n",
    "        # 计算每个候选词组与文档的余弦相似度\n",
    "        similarities = cosine_similarity(candidate_embeddings, doc_embedding).flatten()\n",
    "\n",
    "        # 根据相似度对候选词组排序\n",
    "        keywords = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 显示提取的关键词（限制数量）\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            if i >= num_keywords:\n",
    "                break\n",
    "            result_text.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "# 创建主窗口\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"500x500\")  # 设置窗口尺寸\n",
    "\n",
    "# 创建一个框架，用于存放设置项\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "\n",
    "# 创建用户输入 ngram 范围和关键词数量的输入框\n",
    "ngram_label = tk.Label(settings_frame, text=\"关键词长度 (n-gram):\")\n",
    "ngram_label.grid(row=0, column=0, padx=5)\n",
    "ngram_entry = tk.Entry(settings_frame, width=5)\n",
    "ngram_entry.insert(0, \"2\")  # 默认2词\n",
    "ngram_entry.grid(row=0, column=1, padx=5)\n",
    "\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "# 创建按钮进行关键词提取\n",
    "extract_button = tk.Button(root, text=\"提取关键词\", command=extract_keywords, width=20)\n",
    "extract_button.pack(pady=10)\n",
    "\n",
    "# 创建显示结果的文本框并添加滚动条\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar.set)\n",
    "scrollbar.config(command=result_text.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# 运行主循环\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a2717-045e-4b64-a227-8188d174e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#终版前的最后一版，已经完成了三项模型的整合，并实现了高亮标注功能\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 加载三个不同的预训练模型\n",
    "sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# 初始化全局变量存储原文本内容\n",
    "original_text = \"\"\n",
    "\n",
    "def highlight_keywords(keywords):\n",
    "    \"\"\"在原文本中高亮显示关键词\"\"\"\n",
    "    result_text.tag_remove(\"highlight\", \"1.0\", tk.END)  # 清除已有的高亮\n",
    "    for keyword, _ in keywords:\n",
    "        start = \"1.0\"\n",
    "        while True:\n",
    "            start = result_text.search(keyword, start, stopindex=tk.END, nocase=True)\n",
    "            if not start:\n",
    "                break\n",
    "            end = f\"{start}+{len(keyword)}c\"\n",
    "            result_text.tag_add(\"highlight\", start, end)\n",
    "            start = end\n",
    "    result_text.tag_config(\"highlight\", background=\"yellow\", foreground=\"black\")\n",
    "\n",
    "def extract_keywords(method):\n",
    "    global original_text\n",
    "    try:\n",
    "        ngram_range = int(ngram_entry.get())\n",
    "        num_keywords = int(num_keywords_entry.get())\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"错误\", \"请输入有效的整数\")\n",
    "        return\n",
    "\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\n",
    "\n",
    "    if not file_path:\n",
    "        messagebox.showwarning(\"警告\", \"未选择任何文件\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            original_text = file.read()\n",
    "\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, ngram_range), stop_words='english').fit([original_text])\n",
    "        candidates = vectorizer.get_feature_names_out()\n",
    "\n",
    "        if method == 'sentence_transformers':\n",
    "            candidate_embeddings = sentence_transformer_model.encode(candidates)\n",
    "            doc_embedding = sentence_transformer_model.encode([original_text]).reshape(1, -1)\n",
    "\n",
    "        elif method == 'universal_sentence_encoder':\n",
    "            candidate_embeddings = use_model(candidates)\n",
    "            doc_embedding = use_model([original_text])\n",
    "\n",
    "        elif method == 'bert':\n",
    "            inputs = bert_tokenizer(original_text, return_tensors='tf', truncation=True, padding=True, max_length=512)\n",
    "            outputs = bert_model(inputs)\n",
    "            sequence_output = outputs.last_hidden_state.numpy()[0]\n",
    "\n",
    "            word_ids = inputs['input_ids'][0].numpy()\n",
    "            special_tokens = {bert_tokenizer.cls_token_id, bert_tokenizer.sep_token_id, bert_tokenizer.pad_token_id}\n",
    "            id2word = {idx: bert_tokenizer.convert_ids_to_tokens([word_id])[0] for idx, word_id in enumerate(word_ids) if word_id not in special_tokens}\n",
    "            \n",
    "            valid_indices = [idx for idx in id2word if idx < len(sequence_output)]\n",
    "            word_vectors = np.array([sequence_output[idx] for idx in valid_indices])\n",
    "            id2word = {idx: id2word[idx] for idx in valid_indices}\n",
    "            \n",
    "            doc_vector = np.mean(sequence_output, axis=0).reshape(1, -1)\n",
    "            similarities = cosine_similarity(word_vectors, doc_vector).flatten()\n",
    "            keywords = sorted([(id2word[idx], similarities[i]) for i, idx in enumerate(valid_indices)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # 展示原文本和关键词\n",
    "            result_text.delete('1.0', tk.END)\n",
    "            result_text.insert(tk.END, original_text)\n",
    "            highlight_keywords(keywords[:num_keywords])\n",
    "\n",
    "            keyword_results.delete('1.0', tk.END)\n",
    "            for i, (keyword, score) in enumerate(keywords):\n",
    "                if i >= num_keywords:\n",
    "                    break\n",
    "                keyword_results.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "            return\n",
    "\n",
    "        similarities = cosine_similarity(candidate_embeddings, doc_embedding).flatten()\n",
    "        keywords = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 展示原文本和关键词\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        result_text.insert(tk.END, original_text)\n",
    "        highlight_keywords(keywords[:num_keywords])\n",
    "\n",
    "        keyword_results.delete('1.0', tk.END)\n",
    "        for i, (keyword, score) in enumerate(keywords):\n",
    "            if i >= num_keywords:\n",
    "                break\n",
    "            keyword_results.insert(tk.END, f\"{keyword}: {score:.4f}\\n\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"错误\", f\"处理文件时出现问题：{e}\")\n",
    "\n",
    "# 主窗口和设置项\n",
    "root = tk.Tk()\n",
    "root.title(\"关键词提取工具\")\n",
    "root.geometry(\"600x800\")\n",
    "\n",
    "settings_frame = tk.Frame(root)\n",
    "settings_frame.pack(pady=10)\n",
    "\n",
    "ngram_label = tk.Label(settings_frame, text=\"关键词长度 (n-gram):\")\n",
    "ngram_label.grid(row=0, column=0, padx=5)\n",
    "ngram_entry = tk.Entry(settings_frame, width=5)\n",
    "ngram_entry.insert(0, \"2\")\n",
    "ngram_entry.grid(row=0, column=1, padx=5)\n",
    "\n",
    "num_keywords_label = tk.Label(settings_frame, text=\"提取关键词数量:\")\n",
    "num_keywords_label.grid(row=1, column=0, padx=5)\n",
    "num_keywords_entry = tk.Entry(settings_frame, width=5)\n",
    "num_keywords_entry.insert(0, \"5\")\n",
    "num_keywords_entry.grid(row=1, column=1, padx=5)\n",
    "\n",
    "# 不同提取方法的按钮\n",
    "tk.Button(root, text=\"Sentence Transformers\", command=lambda: extract_keywords('sentence_transformers'), width=20).pack(pady=5)\n",
    "tk.Button(root, text=\"Universal Sentence Encoder\", command=lambda: extract_keywords('universal_sentence_encoder'), width=20).pack(pady=5)\n",
    "tk.Button(root, text=\"BERT\", command=lambda: extract_keywords('bert'), width=20).pack(pady=5)\n",
    "\n",
    "# 原文本显示区域\n",
    "text_frame = tk.Frame(root)\n",
    "text_frame.pack(pady=10)\n",
    "scrollbar1 = tk.Scrollbar(text_frame, orient=\"vertical\")\n",
    "result_text = tk.Text(text_frame, height=15, width=60, yscrollcommand=scrollbar1.set)\n",
    "scrollbar1.config(command=result_text.yview)\n",
    "scrollbar1.pack(side=\"right\", fill=\"y\")\n",
    "result_text.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# 关键词显示区域\n",
    "keyword_frame = tk.Frame(root)\n",
    "keyword_frame.pack(pady=10)\n",
    "scrollbar2 = tk.Scrollbar(keyword_frame, orient=\"vertical\")\n",
    "keyword_results = tk.Text(keyword_frame, height=10, width=60, yscrollcommand=scrollbar2.set)\n",
    "scrollbar2.config(command=keyword_results.yview)\n",
    "scrollbar2.pack(side=\"right\", fill=\"y\")\n",
    "keyword_results.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
