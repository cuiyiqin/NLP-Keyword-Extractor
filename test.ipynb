{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6f12ad-8796-4b66-9868-931be4c24fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 将工作目录切换到 clean.py 所在的目录\n",
    "os.chdir(r'C:\\Users\\ASUS\\TensorFlow Keywords\\clean-text-main\\cleantext')\n",
    "\n",
    "# 将当前工作目录添加到 Python 解释器的搜索路径\n",
    "sys.path.append(r'C:\\Users\\ASUS\\TensorFlow Keywords\\clean-text-main\\cleantext')\n",
    "\n",
    "from clean import clean\n",
    "\n",
    "data_dir = r\"C:\\Users\\ASUS\\TensorFlow Keywords\\test\"\n",
    "\n",
    "# 创建一个新的目录来保存清洗后的文本数据\n",
    "clean_data_dir =  r\"C:\\Users\\ASUS\\TensorFlow Keywords\\test_clean\"\n",
    "os.makedirs(clean_data_dir, exist_ok=True)\n",
    "\n",
    "# 遍历目标地址中的所有文件夹及其子目录\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file_name in files:\n",
    "        if not file_name.startswith('.'):  # 排除隐藏文件\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            # 读取文件内容\n",
    "            with open(file_path, \"r\", encoding=\"latin1\") as file:\n",
    "                text = file.read()\n",
    "            # 对文本进行清洗\n",
    "            clean_text = clean(text, no_line_breaks=True, fix_unicode=True, to_ascii=True, lower=True, no_urls=True, no_emails=True, no_phone_numbers=True, no_numbers=True, no_digits=True, no_currency_symbols=True, no_punct=True, no_emoji=True, remove_stopwords=True)\n",
    "            # 构建清洗后的文件路径\n",
    "            relative_path = os.path.relpath(file_path, data_dir)\n",
    "            clean_file_path = os.path.join(clean_data_dir, relative_path)\n",
    "            os.makedirs(os.path.dirname(clean_file_path), exist_ok=True)\n",
    "            # 将清洗后的文本保存到新文件中\n",
    "            with open(clean_file_path, \"w\", encoding=\"utf-8\") as clean_file:\n",
    "                clean_file.write(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3aa97f-1447-4d45-807c-3a4167a5ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████| 220/220 [00:00<00:00, 1923.32word/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████| 1142/1142 [00:02<00:00, 495.32word/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████| 2709/2709 [00:08<00:00, 325.10word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 381/381 [00:00<00:00, 1435.34word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 145/145 [00:00<00:00, 2844.49word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 4137.29word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 442/442 [00:00<00:00, 1392.34word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 5760.34word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 135/135 [00:00<00:00, 2476.06word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 225/225 [00:00<00:00, 1959.39word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 5341.70word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 64/64 [00:00<00:00, 3730.76word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 59/59 [00:00<00:00, 2704.08word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 3430.06word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 208/208 [00:00<00:00, 1676.90word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 226/226 [00:00<00:00, 1881.08word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 136/136 [00:00<00:00, 3091.40word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 95/95 [00:00<00:00, 3843.16word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 126/126 [00:00<00:00, 2895.63word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 185/185 [00:00<00:00, 2393.07word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 3351.80word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 8720.42word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 87/87 [00:00<00:00, 2726.18word/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████| 786/786 [00:00<00:00, 914.74word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 58/58 [00:00<00:00, 5488.44word/s]\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 3039.92word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 59/59 [00:00<00:00, 5000.69word/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████| 54/54 [00:00<00:00, 5414.46word/s]\n",
      "Training:  40%|███████████████████████████                                         | 39/98 [00:00<00:00, 2675.99word/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m left_context \u001b[38;5;241m=\u001b[39m split_text[\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, index \u001b[38;5;241m-\u001b[39m window):index]\n\u001b[0;32m     41\u001b[0m right_context \u001b[38;5;241m=\u001b[39m split_text[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(split_text), index \u001b[38;5;241m+\u001b[39m window \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m---> 42\u001b[0m avg_vector \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_average_vector_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_score(avg_vector)\n\u001b[0;32m     44\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msoftmax(x_hat)\n",
      "File \u001b[1;32m~\\TensorFlow Keywords\\CBOW_master\\cbow_model.py:53\u001b[0m, in \u001b[0;36mCBOW.get_average_vector_context\u001b[1;34m(self, left_context, right_context)\u001b[0m\n\u001b[0;32m     51\u001b[0m     current_word_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_index[word]\n\u001b[0;32m     52\u001b[0m     one_hot_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_one_hot(current_word_index)\n\u001b[1;32m---> 53\u001b[0m     avg_vector \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation_from_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_hot_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m avg_vector \u001b[38;5;241m=\u001b[39m avg_vector \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow) \u001b[38;5;66;03m#将平均向量除以上下文窗口的总大小，以获得上下文的平均表示。这里假设左右上下文窗口大小相等，因此乘以2。\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_vector\n",
      "File \u001b[1;32m~\\TensorFlow Keywords\\CBOW_master\\cbow_model.py:36\u001b[0m, in \u001b[0;36mCBOW.get_representation_from_onehot\u001b[1;34m(self, one_hot)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_representation_from_onehot\u001b[39m(\u001b[38;5;28mself\u001b[39m, one_hot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    使用one-hot编码得到向量表示\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m temp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\ASUS\\TensorFlow Keywords\\CBOW_master')\n",
    "import os\n",
    "import nltk\n",
    "from cbow_model import CBOW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取文本数据\n",
    "input_folder = 'C:/Users/ASUS/TensorFlow Keywords/test_clean/'\n",
    "\n",
    "# 创建 word_vector 文件夹以保存模型文件\n",
    "output_model_folder = 'C:/Users/ASUS/TensorFlow Keywords/test_vector'\n",
    "os.makedirs(output_model_folder, exist_ok=True)\n",
    "\n",
    "# 遍历每个子文件夹中的所有文件\n",
    "for category in os.listdir(input_folder):\n",
    "    category_path = os.path.join(input_folder, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        for file_name in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                # 读取文件内容\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "\n",
    "                # 创建 CBOW 模型实例\n",
    "                model = CBOW(text=text)\n",
    "\n",
    "                # 设置训练参数\n",
    "                window = 2\n",
    "                n = 100\n",
    "                learning_rate = 0.0001\n",
    "\n",
    "                # 分割文本\n",
    "                split_text = model.text.lower().split()\n",
    "\n",
    "                # 训练模型\n",
    "                for center_word in tqdm(split_text, desc='Training', unit='word'):\n",
    "                    index = split_text.index(center_word)\n",
    "                    left_context = split_text[max(0, index - window):index]\n",
    "                    right_context = split_text[index + 1:min(len(split_text), index + window + 1)]\n",
    "                    avg_vector = model.get_average_vector_context(left_context, right_context)\n",
    "                    x_hat = model.get_score(avg_vector)\n",
    "                    y_hat = model.softmax(x_hat)\n",
    "                    center_word_index = model.word_index[center_word]\n",
    "                    center_word_one_hot_encoding = model.get_one_hot(center_word_index)\n",
    "                    error = model.compute_cross_entropy_error(center_word_one_hot_encoding, y_hat)\n",
    "                    model.update_U(error, avg_vector)\n",
    "                    model.update_V(error, left_context, right_context)\n",
    "\n",
    "                # 保存模型文件\n",
    "                output_model_file = os.path.join(output_model_folder, f\"{file_name}.bin\")\n",
    "                model.save_model(output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ff019-4f0c-4bba-8d20-7f09666cbdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
