{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2c16f6-8fe6-4eb9-af5c-5b7ab63a182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.6 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.6 kB 660.6 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 60.6/60.6 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.4.2-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.6 MB 3.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/10.6 MB 4.7 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/10.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/10.6 MB 8.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/10.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.3/10.6 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.3/10.6 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.4/10.6 MB 14.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.6/10.6 MB 15.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.7/10.6 MB 16.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.6 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.6 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 19.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.7/46.2 MB 15.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.7/46.2 MB 18.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 2.7/46.2 MB 19.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.8/46.2 MB 20.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.8/46.2 MB 20.7 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 5.7/46.2 MB 20.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 6.8/46.2 MB 20.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 7.9/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.9/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 10.0/46.2 MB 21.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 11.0/46.2 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.6/46.2 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 12.6/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.6/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.8/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.8/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.9/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 19.0/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 20.1/46.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 21.2/46.2 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 22.4/46.2 MB 22.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 23.6/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 24.7/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 25.7/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.7/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 27.9/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.0/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 30.1/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.2/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 32.3/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 33.4/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 34.6/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 35.7/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.8/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.9/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 39.1/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 40.1/46.2 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.9/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.9/46.2 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.0/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.1/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.2/46.2 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.4.2 scipy-1.13.0 threadpoolctl-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12811c40-4400-41a7-bd3e-1e6b23b9e30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/137.6 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/137.6 kB ? eta -:--:--\n",
      "     ---------------------- -------------- 81.9/137.6 kB 762.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 137.6/137.6 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/9.0 MB 4.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.5/9.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/9.0 MB 8.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/9.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.0/9.0 MB 12.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.9/9.0 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.1/9.0 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.2/9.0 MB 16.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.4/9.0 MB 17.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.6/9.0 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 388.9/388.9 kB 25.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 287.9/287.9 kB 18.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 15.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 14.2 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 172.0/172.0 kB 10.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a57e2c7-b33b-46ae-96d9-d6213c4438d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\ASUS\\TensorFlow Keywords\\uncased_L-2_H-128_A-2 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\TensorFlow Keywords\\\\cleaned_data\\\\10000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeywords extracted from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcbow_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 56\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(cbow_file)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m text_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cleaned_data_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     57\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     58\u001b[0m keywords \u001b[38;5;241m=\u001b[39m extract_keywords(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\TensorFlow Keywords\\\\cleaned_data\\\\10000.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "\n",
    "# 加载CBOW模型文件\n",
    "def load_cbow_model(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        cbow_model = f.read()  # 读取CBOW模型文件内容\n",
    "    return cbow_model\n",
    "\n",
    "# 加载BERT-Tiny模型\n",
    "def load_bert_model(model_path):\n",
    "    config = BertConfig.from_json_file(os.path.join(model_path, 'bert_config.json'))\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "    return model\n",
    "\n",
    "# 文本处理和关键词提取\n",
    "def extract_keywords(text):\n",
    "    # 使用TF-IDF算法提取关键词\n",
    "    vectorizer = TfidfVectorizer(max_features=10)  # 提取前10个关键词\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keywords = [feature_names[idx] for idx in tfidf_matrix.indices]\n",
    "    return keywords\n",
    "\n",
    "# 获取CBOW模型文件路径列表\n",
    "def get_cbow_files(folder_path):\n",
    "    cbow_files = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.bin'):\n",
    "            cbow_files.append(os.path.join(folder_path, file_name))\n",
    "    return cbow_files\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 加载BERT-Tiny模型\n",
    "    bert_model_path = r'C:\\Users\\ASUS\\TensorFlow Keywords\\uncased_L-2_H-128_A-2'\n",
    "    bert_model = load_bert_model(bert_model_path)\n",
    "\n",
    "    # CBOW模型文件存储文件夹路径\n",
    "    cbow_folder_path = r'C:\\Users\\ASUS\\TensorFlow Keywords\\word_vector'\n",
    "    \n",
    "    # 获取CBOW模型文件路径列表\n",
    "    cbow_files = get_cbow_files(cbow_folder_path)\n",
    "    \n",
    "    # 循环处理每篇文章\n",
    "    for cbow_file in cbow_files:\n",
    "        # 加载CBOW模型\n",
    "        cbow_model = load_cbow_model(cbow_file)\n",
    "\n",
    "        # 文本处理和关键词提取\n",
    "        cleaned_data_path = r'C:\\Users\\ASUS\\TensorFlow Keywords\\cleaned_data'\n",
    "        file_name = os.path.basename(cbow_file).split('.')[0]\n",
    "        text_file = os.path.join(cleaned_data_path, f\"{file_name}.txt\")\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        keywords = extract_keywords(text)\n",
    "\n",
    "        # 打印提取的关键词\n",
    "        print(f\"Keywords extracted from {cbow_file}: {keywords}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033dd9b-0803-4465-b435-c030dbb59630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
