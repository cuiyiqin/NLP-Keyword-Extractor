dominant sequence transduction models based complex recurrent convolutional neural networks include encoder decoder best performing models also connect encoder decoder attention mechanism propose new simple network architecture transformerbased solely attention mechanisms dispensing recurrence convolutions entirely experiments two machine translation tasks show models superior quality parallelizable requiring significantly less time train model achieves <number> bleu wmt <number> englishtogerman translation task improving existing best results including ensembles <number> bleu wmt <number> englishtofrench translation task model establishes new singlemodel stateoftheart bleu score <number> training <number> days eight gpus small fraction training costs best models literature show transformer generalizes well tasks applying successfully english constituency parsing large limited training data